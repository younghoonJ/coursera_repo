{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO29OCZq6m4b"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZUhX9QP6m4d",
        "outputId": "a2bb91d6-6b04-4e54-b917-30b4196db7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D89SustS6m4d"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhvW2guJ6m4e"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "6mleq8ZR6m4e",
        "outputId": "02d857e7-1871-4f12-c5bb-7e7788bc4f1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f08abbda550>"
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgM0LSl46m4f"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zp_BfgT6m4f"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DC_P1Nrv6m4g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UmxUkjNL6m4g"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(state_dim[0], 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, n_actions),\n",
        "    nn.ReLU()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UafE6y2m6m4g"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47C5myFn6m4h"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HS4Y04N56m4h"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    with torch.no_grad():\n",
        "        pred = model(torch.Tensor(states))\n",
        "        prob = nn.functional.softmax(pred, dim=1)\n",
        "    return prob.data.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5KBPjXe66m4h"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSShQrN06m4h"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-ahiFg8z6m4i"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AIVHxqhh6m4i"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3DF5Uh16m4j"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LOwybb-t6m4j"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    cum_rewards = []\n",
        "    prev = 0\n",
        "    for r in reversed(rewards):\n",
        "        prev = r + gamma * prev\n",
        "        cum_rewards.append(prev)\n",
        "    cum_rewards.reverse()\n",
        "    return cum_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itCFmMm46m4j",
        "outputId": "bfc38ec1-e898-494f-c118-68beb8d35592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vao1aWWO6m4j"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f-NePbA56m4k"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JJZmX35_6m4k"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = - torch.sum(torch.sum(probs * log_probs, 1))\n",
        "    \n",
        "    J = torch.mean(log_probs_for_actions * cumulative_returns)\n",
        "    loss = -J + entropy_coef * entropy\n",
        "\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Owz6LVg6m4k"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFB60RK_6m4k",
        "outputId": "2e857ae6-6f41-47f1-b51e-010eaadddb2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean reward:141.880\n",
            "mean reward:107.310\n",
            "mean reward:117.430\n",
            "mean reward:132.840\n",
            "mean reward:118.380\n",
            "mean reward:51.150\n",
            "mean reward:131.570\n",
            "mean reward:161.720\n",
            "mean reward:141.970\n",
            "mean reward:133.920\n",
            "mean reward:143.220\n",
            "mean reward:185.450\n",
            "mean reward:122.990\n",
            "mean reward:122.190\n",
            "mean reward:108.180\n",
            "mean reward:175.760\n",
            "mean reward:555.660\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq6Qh5EM6m4k"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZKpyDBx76m4k"
      },
      "outputs": [],
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "resources": {
            "http://localhost:8080/videos/openaigym.video.2.59.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "ugkuBzRe6m4l",
        "outputId": "36b6304d-c0b3-42e9-f2cb-03bca74d2ffe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.2.59.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "cell_type": "code",
            "metadata": {
              "id": "4a_9rTwz6m4l",
              "outputId": "9ec76de2-c2d6-43e6-9174-42e2b8a6bfec",
              "colab": {
                "base_uri": "https://localhost:8080/"
              }
            },
            "source": [
              "from submit import submit_cartpole\n",
              "submit_cartpole(generate_session, '', '')"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jJg6fY46m4m"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
